{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6912943",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ad5c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Semantic_Segmentation\\offroad-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d57e9b",
   "metadata": {},
   "source": [
    "## 2. Define Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27334374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'img_size': (512, 512), 'num_classes': 10, 'batch_size': 16, 'num_workers': 0, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "TRAIN_IMG_PATH = r'D:\\Semantic_Segmentation\\Offroad_Segmentation_Training_Dataset\\Offroad_Segmentation_Training_Dataset\\train\\Color_Images'\n",
    "TRAIN_MASK_PATH = r'D:\\Semantic_Segmentation\\Offroad_Segmentation_Training_Dataset\\Offroad_Segmentation_Training_Dataset\\train\\Segmentation'\n",
    "VAL_IMG_PATH = r'D:\\Semantic_Segmentation\\Offroad_Segmentation_Training_Dataset\\Offroad_Segmentation_Training_Dataset\\val\\Color_Images'\n",
    "VAL_MASK_PATH = r'D:\\Semantic_Segmentation\\Offroad_Segmentation_Training_Dataset\\Offroad_Segmentation_Training_Dataset\\val\\Segmentation'\n",
    "\n",
    "# Class mapping\n",
    "CLASS_MAPPING = {\n",
    "    100: 0,    # Trees\n",
    "    200: 1,    # Lush Bushes\n",
    "    300: 2,    # Dry Grass\n",
    "    500: 3,    # Dry Bushes\n",
    "    550: 4,    # Ground Clutter\n",
    "    600: 5,    # Flowers\n",
    "    700: 6,    # Logs\n",
    "    800: 7,    # Rocks\n",
    "    7100: 8,   # Landscape\n",
    "    10000: 9   # Sky\n",
    "}\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: 'Trees',\n",
    "    1: 'Lush Bushes',\n",
    "    2: 'Dry Grass',\n",
    "    3: 'Dry Bushes',\n",
    "    4: 'Ground Clutter',\n",
    "    5: 'Flowers',\n",
    "    6: 'Logs',\n",
    "    7: 'Rocks',\n",
    "    8: 'Landscape',\n",
    "    9: 'Sky'\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'img_size': (512, 512),\n",
    "    'num_classes': 10,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca954c65",
   "metadata": {},
   "source": [
    "## 3. Calculate Normalization Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4680fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating normalization parameters...\n",
      "\n",
      "Normalization Statistics:\n",
      "Mean: [0.51130156 0.42428829 0.3174731 ]\n",
      "Std: [0.30373993 0.27931814 0.27089236]\n",
      "\n",
      "Normalization Statistics:\n",
      "Mean: [0.51130156 0.42428829 0.3174731 ]\n",
      "Std: [0.30373993 0.27931814 0.27089236]\n"
     ]
    }
   ],
   "source": [
    "def calculate_normalization_params(img_dir, num_samples=100):\n",
    "    \"\"\"Calculate mean and std for normalization\"\"\"\n",
    "    images = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])[:num_samples]\n",
    "    \n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "    \n",
    "    for img_file in images:\n",
    "        img = cv2.imread(os.path.join(img_dir, img_file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img / 255.0\n",
    "        \n",
    "        all_means.append(np.mean(img, axis=(0, 1)))\n",
    "        all_stds.append(np.std(img, axis=(0, 1)))\n",
    "    \n",
    "    mean = np.mean(all_means, axis=0)\n",
    "    std = np.mean(all_stds, axis=0)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# Calculate normalization parameters\n",
    "print(\"Calculating normalization parameters...\")\n",
    "mean, std = calculate_normalization_params(TRAIN_IMG_PATH, num_samples=100)\n",
    "\n",
    "print(f\"\\nNormalization Statistics:\")\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n",
    "\n",
    "NORM_MEAN = mean\n",
    "NORM_STD = std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59eea5",
   "metadata": {},
   "source": [
    "## 4. Augmentation Pipelines\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'][0], CONFIG['img_size'][1]),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.Rotate(limit=45, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'][0], CONFIG['img_size'][1]),\n",
    "    A.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"Data augmentation pipelines created:\")\n",
    "print(f\"- Training augmentations: {len(train_transform)}\")\n",
    "print(f\"- Validation augmentations: {len(val_transform)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2e34b",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865e0679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation pipelines created:\n",
      "- Training augmentations: 9\n",
      "- Validation augmentations: 3\n"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'][0], CONFIG['img_size'][1]),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.Rotate(limit=45, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CONFIG['img_size'][0], CONFIG['img_size'][1]),\n",
    "    A.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"Data augmentation pipelines created:\")\n",
    "print(f\"- Training augmentations: {len(train_transform)}\")\n",
    "print(f\"- Validation augmentations: {len(val_transform)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269040ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegmentationDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Custom dataset for semantic segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        \n",
    "        # Convert mask class IDs to indices\n",
    "        mask_indexed = np.zeros_like(mask, dtype=np.int64)\n",
    "        for class_id, idx_val in CLASS_MAPPING.items():\n",
    "            mask_indexed[mask == class_id] = idx_val\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask_indexed)\n",
    "            image = augmented['image']\n",
    "            mask_indexed = augmented['mask']\n",
    "        else:\n",
    "            mask_indexed = torch.from_numpy(mask_indexed).long()\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask_indexed,\n",
    "            'filename': img_name\n",
    "        }\n",
    "\n",
    "print(\"SegmentationDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b83db7",
   "metadata": {},
   "source": [
    "## 6. Calculate Class Weights for Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86b9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating class weights...\n",
      "\n",
      "Class Weights (for imbalanced dataset):\n",
      "  Trees: 0.1798\n",
      "  Lush Bushes: 0.1071\n",
      "  Dry Grass: 0.0337\n",
      "  Dry Bushes: 0.5791\n",
      "  Ground Clutter: 0.1447\n",
      "  Flowers: 0.2264\n",
      "  Logs: 8.1554\n",
      "  Rocks: 0.5308\n",
      "  Landscape: 0.0260\n",
      "  Sky: 0.0169\n",
      "\n",
      "Class Weights (for imbalanced dataset):\n",
      "  Trees: 0.1798\n",
      "  Lush Bushes: 0.1071\n",
      "  Dry Grass: 0.0337\n",
      "  Dry Bushes: 0.5791\n",
      "  Ground Clutter: 0.1447\n",
      "  Flowers: 0.2264\n",
      "  Logs: 8.1554\n",
      "  Rocks: 0.5308\n",
      "  Landscape: 0.0260\n",
      "  Sky: 0.0169\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(mask_dir, num_classes=10):\n",
    "    \"\"\"Calculate class weights for imbalanced dataset\"\"\"\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    \n",
    "    mask_files = [f for f in os.listdir(mask_dir) if f.endswith('.png')]\n",
    "    \n",
    "    for mask_file in mask_files:\n",
    "        mask = np.array(Image.open(os.path.join(mask_dir, mask_file)))\n",
    "        \n",
    "        # Count pixels for each class\n",
    "        for class_id, class_idx in CLASS_MAPPING.items():\n",
    "            class_counts[class_idx] += np.sum(mask == class_id)\n",
    "    \n",
    "    # Calculate weights (inverse frequency)\n",
    "    total_pixels = class_counts.sum()\n",
    "    class_weights = total_pixels / (class_counts * num_classes + 1e-6)\n",
    "    \n",
    "    # Normalize weights\n",
    "    class_weights = class_weights / class_weights.sum() * num_classes\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Calculate class weights\n",
    "print(\"Calculating class weights...\")\n",
    "class_weights = calculate_class_weights(TRAIN_MASK_PATH)\n",
    "\n",
    "print(\"\\nClass Weights (for imbalanced dataset):\")\n",
    "for class_idx, weight in enumerate(class_weights):\n",
    "    print(f\"  {CLASS_NAMES[class_idx]}: {weight:.4f}\")\n",
    "\n",
    "# Save class weights\n",
    "np.save('d:\\\\Semantic_Segmentation\\\\class_weights.npy', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f06a5",
   "metadata": {},
   "source": [
    "## 7. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626d7536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Training dataset: 2857 samples\n",
      "Validation dataset: 317 samples\n",
      "\n",
      "DataLoader created:\n",
      "Training batches: 179\n",
      "Validation batches: 20\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = SegmentationDataset(\n",
    "    img_dir=TRAIN_IMG_PATH,\n",
    "    mask_dir=TRAIN_MASK_PATH,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    img_dir=VAL_IMG_PATH,\n",
    "    mask_dir=VAL_MASK_PATH,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader created:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fd309",
   "metadata": {},
   "source": [
    "# Visualize augmented samples\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "sample_idx = 0\n",
    "sample = train_dataset[sample_idx]\n",
    "\n",
    "# Show original and 3 augmented versions\n",
    "for aug_idx in range(3):\n",
    "    sample = train_dataset[sample_idx]\n",
    "    \n",
    "    image = sample['image'].numpy().transpose(1, 2, 0)\n",
    "    # Denormalize\n",
    "    image = image * np.array(NORM_STD).reshape(1, 1, 3) + np.array(NORM_MEAN).reshape(1, 1, 3)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    mask = sample['mask'].numpy()\n",
    "    \n",
    "    axes[aug_idx, 0].imshow(image)\n",
    "    axes[aug_idx, 0].set_title(f'Augmented Image {aug_idx+1}')\n",
    "    axes[aug_idx, 0].axis('off')\n",
    "    \n",
    "    # Visualize mask with colors\n",
    "    mask_colored = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, CONFIG['num_classes']))\n",
    "    for c in range(CONFIG['num_classes']):\n",
    "        mask_colored[mask == c] = colors[c, :3]\n",
    "    \n",
    "    axes[aug_idx, 1].imshow(mask_colored)\n",
    "    axes[aug_idx, 1].set_title(f'Augmented Mask {aug_idx+1}')\n",
    "    axes[aug_idx, 1].axis('off')\n",
    "    \n",
    "    axes[aug_idx, 2].imshow(image * 0.6 + mask_colored * 0.4)\n",
    "    axes[aug_idx, 2].set_title(f'Overlay {aug_idx+1}')\n",
    "    axes[aug_idx, 2].axis('off')\n",
    "    \n",
    "    # Show class distribution in mask\n",
    "    unique, counts = np.unique(mask, return_counts=True)\n",
    "    class_dist = []\n",
    "    for c in range(CONFIG['num_classes']):\n",
    "        if c in unique:\n",
    "            idx = np.where(unique == c)[0][0]\n",
    "            class_dist.append(counts[idx])\n",
    "        else:\n",
    "            class_dist.append(0)\n",
    "    \n",
    "    axes[aug_idx, 3].bar(range(CONFIG['num_classes']), class_dist)\n",
    "    axes[aug_idx, 3].set_title(f'Class Distribution {aug_idx+1}')\n",
    "    axes[aug_idx, 3].set_xlabel('Class')\n",
    "    axes[aug_idx, 3].set_ylabel('Pixel Count')\n",
    "    axes[aug_idx, 3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('augmented_samples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Augmented samples visualization saved as 'augmented_samples.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ffe54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82df0d64",
   "metadata": {},
   "source": [
    "## 9. Save Configuration for Next Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42f3ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration saved to d:\\Semantic_Segmentation\\preprocessing_config.json\n",
      "\n",
      "Feature Engineering Complete!\n",
      "Generated files:\n",
      "  - class_weights.npy\n",
      "  - preprocessing_config.json\n",
      "  - augmented_samples.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare configuration for model training\n",
    "config_dict = {\n",
    "    'image_size': CONFIG['img_size'][0],\n",
    "    'num_classes': CONFIG['num_classes'],\n",
    "    'normalize_mean': NORM_MEAN.tolist(),\n",
    "    'normalize_std': NORM_STD.tolist(),\n",
    "    'class_mapping': CLASS_MAPPING,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'batch_size': CONFIG['batch_size']\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = 'd:\\\\Semantic_Segmentation\\\\preprocessing_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f\"✓ Configuration saved to {config_path}\")\n",
    "print(f\"\\nFeature Engineering Complete!\")\n",
    "print(f\"Generated files:\")\n",
    "print(f\"  - class_weights.npy\")\n",
    "print(f\"  - preprocessing_config.json\")\n",
    "print(f\"  - augmented_samples.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offroad-env (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
